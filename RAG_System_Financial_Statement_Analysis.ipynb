{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4253eccc-f7ea-4b05-b384-765ca6d9a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu transformers langchain pdfplumber -q\n",
    "!pip install -U langchain-huggingface -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "460f5902-7b98-4c5a-85a4-52d206a9ecff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abgerufene Chunks für die Frage:\n",
      "Chunk 1:\n",
      ".5 Achieved\n",
      "$ 35.0 Achieved $ 3.0 Achieved\n",
      "$ 55.0 Achieved $ 4.5 Achieved\n",
      "$ 75.0 Achieved $ 6.0 Achieved\n",
      "$ 100.0 - $ 8.0 Achieved\n",
      "$ 125.0 - $ 10.0 Achieved\n",
      "$ 150.0 - $ 12.0 Achieved\n",
      "$ 175.0 - $ 14.0 Achieved\n",
      "Stock-based compensation under the 2018 CEO Performance Award represented a non-cash expense and was recorded as a Selling, general, and\n",
      "administrative operating expense in our consolidated statements of operations. In each quarter since the grant of the 2018 CEO Performance Award, we\n",
      "had recognized expense, generally on a pro-rated basis, for only the number of tranches (up to the maximum of 12 tranches) that corresponded to the\n",
      "number of operational milestones that had been achieved or had been determined probable of being achieved in the future, in accordance with the\n",
      "following principles.\n",
      "On the grant date, a Monte Carlo simulation was used to determine for each tranche (i) a fixed amount of expense for such tranche and (ii) the\n",
      "future time when the market capitalization milest\n",
      "\n",
      "Chunk 2:\n",
      "ect the registrant’s ability to record, process, summarize and report financial information; and\n",
      "(b) Any fraud, whether or not material, that involves management or other employees who have a significant role in the registrant’sinternal control over financial reporting.\n",
      "Date: January 26, 2024 /s/ Elon Musk\n",
      "Elon Musk\n",
      "Chief Executive Officer\n",
      "(Principal Executive Officer)\n",
      "Exhibit 31.2\n",
      "CERTIFICATIONS\n",
      "I, Vaibhav Taneja, certify that:\n",
      "1. I have reviewed this Annual Report on Form 10-K of Tesla, Inc.;\n",
      "2. Based on my knowledge, this report does not contain any untrue statement of a material fact or omit to state a material fact necessary to make the\n",
      "statements made, in light of the circumstances under which such statements were made, not misleading with respect to the period covered by this\n",
      "report;\n",
      "3. Based on my knowledge, the financial statements, and other financial information included in this report, fairly present in all material respects the\n",
      "financial condition, results of operations an\n",
      "\n",
      "Chunk 3:\n",
      "e may have to expend additional cash to compensate our employees and our ability to retain and hire\n",
      "qualified personnel may be harmed.\n",
      "20Table of Contents\n",
      "We are highly dependent on the services of Elon Musk, Technoking of Tesla and our Chief Executive Officer.\n",
      "We are highly dependent on the services of Elon Musk, Technoking of Tesla and our Chief Executive Officer. Although Mr. Musk spends significant\n",
      "time with Tesla and is highly active in our management, he does not devote his full time and attention to Tesla. Mr. Musk also currently serves as Chief\n",
      "Executive Officer and Chief Technical Officer of Space Exploration Technologies Corp., a developer and manufacturer of space launch vehicles, Chairman\n",
      "and Chief Technical Officer of X Corp., a social media company, and is involved in other emerging technology ventures.\n",
      "Our information technology systems or data, or those of our service providers or customers or users could be subject to cyber-\n",
      "attacks or other security incidents, which c\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\phili\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antwort auf die Frage:\n",
      "Elon Musk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import pipeline\n",
    "\n",
    "# Schritt 1: PDF-Dokument 'tsl-10k-report.pdf' extrahieren\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Schritt 2: Text in Chunks aufteilen (für Embeddings)\n",
    "def split_text_into_chunks(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Schritt 3: Embeddings erstellen\n",
    "def create_embeddings(chunks):\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "    embeddings = embeddings_model.embed_documents(chunks)\n",
    "    return np.array(embeddings), embeddings_model\n",
    "\n",
    "# Schritt 4: FAISS Vektordatenbank erstellen\n",
    "def create_faiss_index(embeddings):\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2-Distanz für Ähnlichkeitsvergleich\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# Schritt 5: Frage verarbeiten und Antwort finden\n",
    "def find_answer_in_pdf(question, chunks, index, embeddings_model):\n",
    "    # Frage als Embedding\n",
    "    question_embedding = embeddings_model.embed_query(question)\n",
    "\n",
    "    # Ähnlichste Chunks finden\n",
    "    D, I = index.search(np.array([question_embedding]), k=3)\n",
    "    closest_chunks = [chunks[i] for i in I[0]]\n",
    "\n",
    "    # Gib die abgerufenen Chunks aus\n",
    "    print(\"Abgerufene Chunks für die Frage:\")\n",
    "    for i, chunk in enumerate(closest_chunks):\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(chunk)\n",
    "        print()\n",
    "\n",
    "    # Combine chunks into a single context\n",
    "    context = \" \".join(closest_chunks)\n",
    "\n",
    "    return context, closest_chunks\n",
    "\n",
    "# Schritt 6: Extraktive Antwort mit BERT basierend auf dem Kontext\n",
    "def generate_answer(question, context):\n",
    "    # Verwende BERT für Frage-Antwort-Aufgaben\n",
    "    qa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "    # Frage beantworten mit BERT\n",
    "    result = qa_pipeline({\n",
    "        'question': question,\n",
    "        'context': context\n",
    "    })\n",
    "\n",
    "    return result['answer']\n",
    "\n",
    "# Hauptfunktion zum Ausführen des Projekts\n",
    "def main(pdf_path, question):\n",
    "    # 1. Extrahiere Text aus dem PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 2. Text in Chunks aufteilen\n",
    "    chunks = split_text_into_chunks(text)\n",
    "    \n",
    "    # 3. Embeddings erstellen\n",
    "    embeddings, embeddings_model = create_embeddings(chunks)\n",
    "    \n",
    "    # 4. FAISS Index erstellen\n",
    "    index = create_faiss_index(embeddings)\n",
    "    \n",
    "    # 5. Frage beantworten und Chunks anzeigen\n",
    "    context, closest_chunks = find_answer_in_pdf(question, chunks, index, embeddings_model)\n",
    "    \n",
    "    # 6. Antwort generieren basierend auf dem abgerufenen Kontext mit BERT\n",
    "    answer = generate_answer(question, context)\n",
    "    \n",
    "    return answer, closest_chunks\n",
    "\n",
    "# Setze Umgebungsvariable für Windows Symlink-Cache-Problem\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "# Ausführen\n",
    "pdf_path = \"tsl-10k-report.pdf\"  # Pfad zu deinem PDF-Dokument\n",
    "question = \"Who is the CEO of the company?\"\n",
    "answer, retrieved_chunks = main(pdf_path, question)\n",
    "\n",
    "# Ausgabe der Antwort\n",
    "print(\"Antwort auf die Frage:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582ec1ae-dd60-4669-bafc-21384cda196b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abgerufene Chunks für die Frage:\n",
      "Chunk 1:\n",
      "ect the registrant’s ability to record, process, summarize and report financial information; and\n",
      "(b) Any fraud, whether or not material, that involves management or other employees who have a significant role in the registrant’sinternal control over financial reporting.\n",
      "Date: January 26, 2024 /s/ Elon Musk\n",
      "Elon Musk\n",
      "Chief Executive Officer\n",
      "(Principal Executive Officer)\n",
      "Exhibit 31.2\n",
      "CERTIFICATIONS\n",
      "I, Vaibhav Taneja, certify that:\n",
      "1. I have reviewed this Annual Report on Form 10-K of Tesla, Inc.;\n",
      "2. Based on my knowledge, this report does not contain any untrue statement of a material fact or omit to state a material fact necessary to make the\n",
      "statements made, in light of the circumstances under which such statements were made, not misleading with respect to the period covered by this\n",
      "report;\n",
      "3. Based on my knowledge, the financial statements, and other financial information included in this report, fairly present in all material respects the\n",
      "financial condition, results of operations an\n",
      "\n",
      "Chunk 2:\n",
      ".5 Achieved\n",
      "$ 35.0 Achieved $ 3.0 Achieved\n",
      "$ 55.0 Achieved $ 4.5 Achieved\n",
      "$ 75.0 Achieved $ 6.0 Achieved\n",
      "$ 100.0 - $ 8.0 Achieved\n",
      "$ 125.0 - $ 10.0 Achieved\n",
      "$ 150.0 - $ 12.0 Achieved\n",
      "$ 175.0 - $ 14.0 Achieved\n",
      "Stock-based compensation under the 2018 CEO Performance Award represented a non-cash expense and was recorded as a Selling, general, and\n",
      "administrative operating expense in our consolidated statements of operations. In each quarter since the grant of the 2018 CEO Performance Award, we\n",
      "had recognized expense, generally on a pro-rated basis, for only the number of tranches (up to the maximum of 12 tranches) that corresponded to the\n",
      "number of operational milestones that had been achieved or had been determined probable of being achieved in the future, in accordance with the\n",
      "following principles.\n",
      "On the grant date, a Monte Carlo simulation was used to determine for each tranche (i) a fixed amount of expense for such tranche and (ii) the\n",
      "future time when the market capitalization milest\n",
      "\n",
      "Chunk 3:\n",
      "e may have to expend additional cash to compensate our employees and our ability to retain and hire\n",
      "qualified personnel may be harmed.\n",
      "20Table of Contents\n",
      "We are highly dependent on the services of Elon Musk, Technoking of Tesla and our Chief Executive Officer.\n",
      "We are highly dependent on the services of Elon Musk, Technoking of Tesla and our Chief Executive Officer. Although Mr. Musk spends significant\n",
      "time with Tesla and is highly active in our management, he does not devote his full time and attention to Tesla. Mr. Musk also currently serves as Chief\n",
      "Executive Officer and Chief Technical Officer of Space Exploration Technologies Corp., a developer and manufacturer of space launch vehicles, Chairman\n",
      "and Chief Technical Officer of X Corp., a social media company, and is involved in other emerging technology ventures.\n",
      "Our information technology systems or data, or those of our service providers or customers or users could be subject to cyber-\n",
      "attacks or other security incidents, which c\n",
      "\n",
      "Antwort auf die Frage:\n",
      "Elon Musk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import pipeline\n",
    "\n",
    "# Schritt 1: PDF-Dokument 'tsl-10k-report.pdf' extrahieren\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Schritt 2: Text in Chunks aufteilen (für Embeddings)\n",
    "def split_text_into_chunks(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Schritt 3: Embeddings erstellen\n",
    "def create_embeddings(chunks):\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "    embeddings = embeddings_model.embed_documents(chunks)\n",
    "    return np.array(embeddings), embeddings_model\n",
    "\n",
    "# Schritt 4: FAISS Vektordatenbank erstellen\n",
    "def create_faiss_index(embeddings):\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2-Distanz für Ähnlichkeitsvergleich\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# Schritt 5: Frage verarbeiten und Antwort finden\n",
    "def find_answer_in_pdf(question, chunks, index, embeddings_model):\n",
    "    # Frage als Embedding\n",
    "    question_embedding = embeddings_model.embed_query(question)\n",
    "\n",
    "    # Ähnlichste Chunks finden\n",
    "    D, I = index.search(np.array([question_embedding]), k=3)\n",
    "    closest_chunks = [chunks[i] for i in I[0]]\n",
    "\n",
    "    # Gib die abgerufenen Chunks aus\n",
    "    print(\"Abgerufene Chunks für die Frage:\")\n",
    "    for i, chunk in enumerate(closest_chunks):\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(chunk)\n",
    "        print()\n",
    "\n",
    "    # Combine chunks into a single context\n",
    "    context = \" \".join(closest_chunks)\n",
    "\n",
    "    return context, closest_chunks\n",
    "\n",
    "# Schritt 6: Generative Antwort mit T5 basierend auf dem Kontext\n",
    "def generate_answer(question, context):\n",
    "    # Verwende T5 für generative Frage-Antwort-Aufgaben\n",
    "    generator = pipeline('text2text-generation', model='t5-base')\n",
    "\n",
    "    # Kombiniere die Frage und den Kontext in einem Prompt\n",
    "    prompt_template = f\"question: {question} context: {context}\"\n",
    "\n",
    "    # Generiere mehrere Antworten (begrenzt auf 200 Tokens für neue Tokens)\n",
    "    result = generator(prompt_template, max_length=200, num_return_sequences=1)\n",
    "\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Hauptfunktion zum Ausführen des Projekts\n",
    "def main(pdf_path, question):\n",
    "    # 1. Extrahiere Text aus dem PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 2. Text in Chunks aufteilen\n",
    "    chunks = split_text_into_chunks(text)\n",
    "    \n",
    "    # 3. Embeddings erstellen\n",
    "    embeddings, embeddings_model = create_embeddings(chunks)\n",
    "    \n",
    "    # 4. FAISS Index erstellen\n",
    "    index = create_faiss_index(embeddings)\n",
    "    \n",
    "    # 5. Frage beantworten und Chunks anzeigen\n",
    "    context, closest_chunks = find_answer_in_pdf(question, chunks, index, embeddings_model)\n",
    "    \n",
    "    # 6. Antwort generieren basierend auf dem abgerufenen Kontext mit T5\n",
    "    answer = generate_answer(question, context)\n",
    "    \n",
    "    return answer, closest_chunks\n",
    "\n",
    "# Setze Umgebungsvariable für Windows Symlink-Cache-Problem\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "# Ausführen\n",
    "pdf_path = \"tsl-10k-report.pdf\"  # Pfad zu deinem PDF-Dokument\n",
    "question = \"Who is the CEO of the company\"\n",
    "answer, retrieved_chunks = main(pdf_path, question)\n",
    "\n",
    "# Ausgabe der Antwort\n",
    "print(\"Antwort auf die Frage:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea4591df-fc31-4ea5-aeb3-1cae6fc0430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abgerufene Chunks für die Frage:\n",
      "Chunk 1:\n",
      "ation with the SEC. The information posted on our website\n",
      "is not incorporated by reference into this Annual Report on Form 10-K.\n",
      "ITEM 1A. RISK FACTORS\n",
      "You should carefully consider the risks described below together with the other information set forth in this report, which could materially affect our\n",
      "business, financial condition and future results. The risks described below are not the only risks facing our company. Risks and uncertainties not currently\n",
      "known to us or that we currently deem to be immaterial also may materially adversely affect our business, financial condition and operating results.\n",
      "Risks Related to Our Ability to Grow Our Business\n",
      "We may experience delays in launching and ramping the production of our products and features, or we may be unable to control\n",
      "our manufacturing costs.\n",
      "We have previously experienced and may in the future experience launch and production ramp delays for new products and features. For example,\n",
      "we encountered unanticipated supplier issues tha\n",
      "\n",
      "Chunk 2:\n",
      "tion and oversight of applicable third-party service providers and potential fourth-party risks when handling and/or processing\n",
      "our employee, business or customer data. In addition to new vendor onboarding, we perform risk management during third-party cybersecurity\n",
      "compromise incidents to identify and mitigate risks to us from third-party incidents.\n",
      "We describe whether and how risks from identified cybersecurity threats, including as a result of any previous cybersecurity incidents, have\n",
      "materially affected or are reasonably likely to materially affect us, including our business strategy, results of operations, or financial condition, under the\n",
      "heading “Our information technology systems or data, or those of our service providers or customers or users could be subject to cyber-attacks or other\n",
      "security incidents, which could result in data breaches, intellectual property theft, claims, litigation, regulatory investigations, significant liability,\n",
      "reputational damage and other adverse \n",
      "\n",
      "Chunk 3:\n",
      "and/or reputational harm.\n",
      "We also rely on service providers, and similar incidents relating to their information technology systems could also have a material adverse effect on\n",
      "our business. There have been and may continue to be significant supply chain attacks. Our service providers, including our workforce management\n",
      "software provider, have been subject to ransomware and other security incidents, and we cannot guarantee that our or our service providers’ systems\n",
      "have not been breached or that they do not contain exploitable defects, bugs, or vulnerabilities that could result in a security incident, or other disruption\n",
      "to, our or our service providers’ systems. Our ability to monitor our service providers’ security measures is limited, and, in any event, malicious third\n",
      "parties may be able to circumvent those security measures.\n",
      "Further, the implementation, maintenance, segregation and improvement of these systems require significant management time, support and cost,\n",
      "and there are in\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b410b0265b49a18028e4705f1c418a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/930 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phili\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\phili\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-j-6B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5b54722f50463493e0ef8702ce40a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Schritt 1: PDF-Dokument 'tsl-10k-report.pdf' extrahieren\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Schritt 2: Text in Chunks aufteilen (für Embeddings)\n",
    "def split_text_into_chunks(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Schritt 3: Embeddings erstellen\n",
    "def create_embeddings(chunks):\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "    embeddings = embeddings_model.embed_documents(chunks)\n",
    "    return np.array(embeddings), embeddings_model\n",
    "\n",
    "# Schritt 4: FAISS Vektordatenbank erstellen\n",
    "def create_faiss_index(embeddings):\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2-Distanz für Ähnlichkeitsvergleich\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# Schritt 5: Frage verarbeiten und Antwort finden\n",
    "def find_answer_in_pdf(question, chunks, index, embeddings_model):\n",
    "    # Frage als Embedding\n",
    "    question_embedding = embeddings_model.embed_query(question)\n",
    "\n",
    "    # Ähnlichste Chunks finden\n",
    "    D, I = index.search(np.array([question_embedding]), k=3)\n",
    "    closest_chunks = [chunks[i] for i in I[0]]\n",
    "\n",
    "    # Gib die abgerufenen Chunks aus\n",
    "    print(\"Abgerufene Chunks für die Frage:\")\n",
    "    for i, chunk in enumerate(closest_chunks):\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(chunk)\n",
    "        print()\n",
    "\n",
    "    # Combine chunks into a single context\n",
    "    context = \" \".join(closest_chunks)\n",
    "\n",
    "    return context, closest_chunks\n",
    "\n",
    "# Schritt 7: Generative Antwort mit GPT-J basierend auf dem Kontext\n",
    "def generate_answer(question, context):\n",
    "    # Verwende GPT-J durch Hugging Face Transformers\n",
    "    generator = pipeline('text-generation', model='EleutherAI/gpt-j-6B')\n",
    "\n",
    "    # Kombiniere Frage und Kontext in einem Prompt\n",
    "    prompt_template = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    # Generiere die Antwort\n",
    "    result = generator(prompt_template, max_length=200, temperature=0.7, num_return_sequences=1)\n",
    "    \n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Hauptfunktion zum Ausführen des Projekts\n",
    "def main(pdf_path, question):\n",
    "    # 1. Extrahiere Text aus dem PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 2. Text in Chunks aufteilen\n",
    "    chunks = split_text_into_chunks(text)\n",
    "    \n",
    "    # 3. Embeddings erstellen\n",
    "    embeddings, embeddings_model = create_embeddings(chunks)\n",
    "    \n",
    "    # 4. FAISS Index erstellen\n",
    "    index = create_faiss_index(embeddings)\n",
    "    \n",
    "    # 5. Frage beantworten und Chunks anzeigen\n",
    "    context, closest_chunks = find_answer_in_pdf(question, chunks, index, embeddings_model)\n",
    "    \n",
    "    # 6. Antwort generieren basierend auf dem abgerufenen Kontext mit GPT-J\n",
    "    answer = generate_answer(question, context)\n",
    "    \n",
    "    return answer, closest_chunks\n",
    "\n",
    "# Ausführen\n",
    "pdf_path = \"tsl-10k-report.pdf\"  # Pfad zu deinem PDF-Dokument\n",
    "question = \"Give me three company risks\"\n",
    "answer, retrieved_chunks = main(pdf_path, question)\n",
    "\n",
    "# Ausgabe der Antwort\n",
    "print(\"Antwort auf die Frage:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbea9b0-88a4-4451-9cd9-3cb771d6a98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
